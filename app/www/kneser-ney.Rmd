---
output:
  bookdown::html_document2:
    highlight: zenburn
    keep_md: yes
    theme: null
    toc: no
    self_contained: yes
bibliography: publications.bib
---

## Introduction {-}
This document makes heavy use of the paper by @Chen1996 and @Brants:2007aa.

The goal of the prediction algorithms described below is calculate the probability of a word $w_i$ given some previous words $w_{i-3}, w_{i-2}, w_{i-1}$ (like for example three words).
This probability can be written as $p(w_i|w_{i-3} w_{i-2} w_{i-1})$.

The following explanations use some common defintions:  
$w_{k}^{l}$ is defined as the sequence of words $w_k \cdots w_l$.
The count of $w_{k}^{l}$ is defined as $c(w_{k}^{l})$ being the number of occurences of $w_{k}^{l}$ in the training set.



## Stupid Back-Off Predictions {-}
Traditional algorithms try to compute a proper probability of an n-gram -- like for example the Kneser-Ney approach outlined below -- which takes the form

\begin{equation}
p(w_i|w_{i-n+1}^{i-1}) = 
\begin{cases}
\hat{p}(w_{i-n+1}^{i}) &\text{ if }w_{i-n+1}^{i}\text{ is found,} \\
\lambda(w_{i-n+1}^{i-1})p(w_{i-n+2}^{i}) & \text{ otherwise}
\end{cases}
\end{equation}

with some precomputed probability $\hat{p}(\cdot)$ and back-off weights $\lambda(\cdot)$.

The stupid back-off approach weakens the probability assumption but in return introduces a much simpler computational algorithm.
Here, the counts of the n-grams are used to calculate a score

\begin{equation}
s(w_i|w_{i-n+1}^{i-1}) = 
\begin{cases}
\frac{c(w_{i-n+1}^{i})}{c(w_{i-n+1}^{i-1})} &\text{ if }c(w_{i-n+1}^{i})>0 \\
\alpha s(w_i|w_{i-n+2}^{i-1}) & \text{ otherwise}
\end{cases}
\end{equation}

To compute the score for single words, i.e. unigrams, the score is defined as

\begin{equation}
s(w_i) = \frac{c(w_i)}{N}
\end{equation}

where $N$ is the size of the training set, meaning the number of words in the training set.
The authors propose a fixed back-off factor of $\alpha=0.4$.


## Absolute Discounting {-}
For every higher-order n-gram a fixed discount $0\leq D\leq 1$ is subtracted from each non-zero count.

\begin{equation}
p_{abs}(w_i|w_{i-n+1}^{i-1}) = \frac{\max\{c(w_{i-n+1}^{i})-D,0\}}{\sum_{w_i}c(w_{i-n+1}^{i})} + (1-\lambda_{w_{i-n+1}^{i-1}})p_{abs}(w_i|w_{i-n+2}^{i-1}) (\#eq:e9)
\end{equation}

To make this sum to one,

\begin{equation}
1-\lambda_{w_{i-n+1}^{i-1}} = \frac{D}{\sum_{w_i}c(w_{i-n+1}^{i})} N_{1+}(w_{i-n+1}^{i-1}\bullet) (\#eq:e10)
\end{equation}

must hold with

\begin{equation}
N_{1+}(w_{i-n+1}^{i-1}\bullet) = \lvert\{w_i: c(w_{i-n+1}^{i-1}w_i)> 0 \}\rvert (\#eq:e6)
\end{equation}

being the number of unique words that follow the history $w_{i-n+1}^{i-1}$.
As a suggestions for $D$ @Neyetal1994 propose

\begin{equation}
D=\frac{n_1}{n_1+2n_2}
\end{equation}

where $n_1$ and $n_2$ are the total number of n-grams with exactly one or two counts.





## Kneser-Ney Smoothing {-}
Analog to \@ref(eq:e9) we have with $0\leq D\leq 1$

\begin{align}
p_{KN}(w_i|w_{i-n+1}^{i-1}) =& \frac{\max\{c(w_{i-n+1}^{i})-D,0\}}{\sum_{w_i}c(w_{i-n+1}^{i})} + \notag\\ &\frac{D}{\sum_{w_i}c(w_{i-n+1}^{i})}N_{1+}(w_{i-n+1}^{i-1}\bullet)p_{KN}(w_i|w_{i-n+2}^{i-1}) (\#eq:e13)
\end{align}

We want that for bigrams the following equation holds:

\begin{equation}
\sum_{w_{i-1}}p_{KN}(w_{i-1}w_i) = \frac{c(w_i)}{\sum_{w_i}c(w_i)} (\#eq:e12)
\end{equation}

The right-hand side of the equation is the frequency of $w_i$ in the training data, whereas the left-hand side id the unigram marginal for $w_i$ of the smoothed bigram distribution $p_{KN}$.

To find such a unigram distribution $p_{KN}(w_i)$ such that \@ref(eq:e12) is fulfilled we expand \@ref(eq:e12) to

\begin{equation}
\frac{c(w_i)}{\sum_{w_i}c(w_i)} = \sum_{w_{i-1}}p_{KN}(w_i|w_{i-1})p(w_{i-1}).
\end{equation}

For $p(w_{i-1})$ the distribution found in the training data is used, which leads to

\begin{equation}
p(w_{i-1}) = \frac{c(w_{i-1})}{\sum_{w_{i-1}}c(w_{i-1})}.
\end{equation}

This can be substituted to

\begin{equation}
c(w_i) = \sum_{w_{i-1}}c(w_{i-1})p_{KN}(w_i|w_{i-1})
\end{equation}

which can again be substituted in \@ref(eq:e13) to obtain

\begin{align}
c(w_i) =& \sum_{w_{i-1}}c(w_{i-1})\left[ \frac{\max\{c(w_{i-1}w_i)-D,0\}}{\sum_{w_i}c(w_{i-1}w_i)} + \frac{D}{\sum_{w_i}c(w_{i-1}w_i)} N_{1+}(w_{i-1}\bullet)p_{KN}(w_i) \right] \\
=& \sum{w_{i-1}:c(w_{i-1}w_i)>0}c(w_{i-1})\frac{c(w_{i-1}w_i)-D}{c(w_{i-1})} \nonumber \\
& + \sum_{w_{i-1}}c(w_{i-1})\frac{D}{c(w_{i-1})}N_{1+}(w_{i-1}\bullet)p_{KN}(w_i) \\
=& c(w_i) - N_{1+}(\bullet w_i) D + D p_{KN}(w_i)\sum_{w_{i-1}} N_{1+}(w_{i-1}\bullet) \\
=& c(w_i) - N_{1+}(\bullet w_i) D + D p_{KN}(w_i) N_{1+}(\bullet\bullet)
\end{align}

where

\begin{equation}
N_{1+}(\bullet w_i) = \lvert \{w_{i-1}: c(w_{i-1} w_i) >0\} \rvert
\end{equation}

is the number of different words $w_{i-1}$ that precede $w_i$ in the training data and where

\begin{equation}
N_{+1}(\bullet\bullet) = \sum_{w_{i-1}}N_{1+}(w_{i-1}\bullet) = \lvert \{(w_{i-1}, w_i): c(w_{i-1} w_i) >0\} \rvert = \sum_{w_i}N_{1+}(\bullet w_i)
\end{equation}

Thus, if we solve for $p_{KN}(w_i)$, we obtain

\begin{equation}
p_{KN}(w_i) = \frac{N_{1+}(\bullet w_i)}{N_{1+}(\bullet\bullet)}.
\end{equation}

For higher-order models the general definition is given by

\begin{equation}
p_{KN}(w_i|w_{i-n+2}^{i-1}) = \frac{N_{1+}(\bullet w_{i-n+2}^i)}{N{1+}(\bullet w_{i-n+2}^{i-1} \bullet)}
\end{equation}

where

\begin{align}
N_{1+}(\bullet w_{i-n+2}^{i}) &= \lvert \{w_{i-n+1}: c(w_{i-n+1}^i)>0\} \rvert \\
N_{1+}(\bullet w_{i-n+2}^{i-1} \bullet) &= \lvert \{(w_{i-n+1}, w_i): c(w_{i-n+1}^i)>0\} \rvert = \sum_{w_i}N_{1+}(\bullet w_{i-n+2}^i).
\end{align}





## Modified Kneser-Ney Smoothing {-}

An improved version of the above method can be achieved by using not one discount $D$, but different depending on the actual count.
We define $D(c)$ as follows

\begin{align}
D(c) &=\begin{cases}
0 & \text{if } c=0 \\
D_1 & \text{if } c=1 \\
D_2 & \text{if } c=2 \\
D_{3+} & \text{if } c\geq 3
\end{cases}
\end{align}

With this we use an adjusted version of \@ref(eq:e13), more precisely

\begin{equation}
p_{KN}(w_i|w_{i-n+1}^{i-1}) = \frac{\max\{c(w_{i-n+1}^{i}) - D(c(w_{i-n+1}^i)),0\}}{\sum_{w_i}c(w_{i-n+1}^{i})} + \gamma(w_{i-n+1}^{i-1}) p_{KN}(w_i|w_{i-n+2}^{i-1})
\end{equation}

with

\begin{equation}
\gamma(w_{i-n+1}^{i-1}) = \frac{D_1 N_{1}(w_{i-n+1}^{i-1} \bullet) + D_2 N_{2}(w_{i-n+1}^{i-1} \bullet) + D_{3+} N_{3+}(w_{i-n+1}^{i-1} \bullet)}{\sum{w_i}c(w_{i-n+1}^{i})}
\end{equation}

An estimate for the optimal values of the discounts can be given by

\begin{align}
D_1 &= 1 - 2Y\frac{n_2}{n_1} \\
D_2 &= 2 - 3Y\frac{n_3}{n_2} \\
D_{3+} &= 3 - 4Y\frac{n_4}{n_3} \\
\text{with } Y &= \frac{n_1}{n_1 + 2n_2}
\end{align}

Note that $n_1\ldots n_4$ is defined for each $n$-gram and, thus, also the $D_1$, $D_2$ and $D_{3+}$ need to be understood as a discount for each $n$ (=$n$-gram "level").




## Accuracy & Timing {-}
Both scores deliver very promising results.

We use the benchmark from [Hern√°n Foffani](https://github.com/hfoffani/dsci-benchmark) to measure accuracy, memory footprint and runtime.

For the modified Kneser-Ney score we get the following picture:
```
Overall top-3 score:     17.67 %
Overall top-1 precision: 12.97 %
Overall top-3 precision: 21.66 %
Average runtime:         3.27 msec
Number of predictions:   28464
Total memory used:       23.07 MB

Dataset details
 Dataset "blogs" (599 lines, 14587 words, hash 14b3c593e543eb8b2932cf00b646ed653e336897a03c82098b725e6e1f9b7aa2)
  Score: 17.39 %, Top-1 precision: 12.79 %, Top-3 precision: 21.34 %
 Dataset "tweets" (793 lines, 14071 words, hash 7fa3bf921c393fe7009bc60971b2bb8396414e7602bb4f409bed78c7192c30f4)
  Score: 17.95 %, Top-1 precision: 13.15 %, Top-3 precision: 21.98 %


R version 3.4.2 (2017-09-28), platform x86_64-apple-darwin15.6.0 (64-bit)
Attached non-base packages:   stringi (v1.1.6), digest (v0.6.12), knitr (v1.17), ggplot2 (v2.2.1), ngram (v3.0.4), tm (v0.7-2), NLP (v0.1-11), markdown (v0.8), dplyr (v0.7.4), data.table (v1.10.4-3), shinydashboard (v0.6.1), shiny (v1.0.5)
Unattached non-base packages: Rcpp (v0.12.14), highr (v0.6), plyr (v1.8.4), compiler (v3.4.2), bindr (v0.1), bitops (v1.0-6), tools (v3.4.2), gtable (v0.2.0), jsonlite (v1.5), evaluate (v0.10.1), tibble (v1.3.4), pkgconfig (v2.0.1), rlang (v0.1.4), rstudioapi (v0.7), yaml (v2.1.14), parallel (v3.4.2), bindrcpp (v0.2), stringr (v1.2.0), xml2 (v1.1.1), revealjs (v0.9), grid (v3.4.2), rprojroot (v1.2), glue (v1.2.0), R6 (v2.2.2), rmarkdown (v1.8), bookdown (v0.5), RJSONIO (v1.3-0), magrittr (v1.5), scales (v0.5.0), backports (v1.1.1), htmltools (v0.3.6), rsconnect (v0.8.5), assertthat (v0.2.0), colorspace (v1.3-2), mime (v0.5), xtable (v1.8-2), httpuv (v1.3.5), RCurl (v1.95-4.8), lazyeval (v0.2.1), munsell (v0.4.3), slam (v0.1-40)
```

The stupid back-off model gave slightly better top-1 scores but had slightly lower top-3 scores.
```
Overall top-3 score:     17.53 %
Overall top-1 precision: 13.02 %
Overall top-3 precision: 21.51 %
Average runtime:         3.33 msec
Number of predictions:   28464
Total memory used:       23.07 MB

Dataset details
 Dataset "blogs" (599 lines, 14587 words, hash 14b3c593e543eb8b2932cf00b646ed653e336897a03c82098b725e6e1f9b7aa2)
  Score: 17.15 %, Top-1 precision: 12.59 %, Top-3 precision: 21.25 %
 Dataset "tweets" (793 lines, 14071 words, hash 7fa3bf921c393fe7009bc60971b2bb8396414e7602bb4f409bed78c7192c30f4)
  Score: 17.91 %, Top-1 precision: 13.45 %, Top-3 precision: 21.76 %


R version 3.4.2 (2017-09-28), platform x86_64-apple-darwin15.6.0 (64-bit)
Attached non-base packages:   stringi (v1.1.6), digest (v0.6.12), knitr (v1.17), ggplot2 (v2.2.1), ngram (v3.0.4), tm (v0.7-2), NLP (v0.1-11), markdown (v0.8), dplyr (v0.7.4), data.table (v1.10.4-3), shinydashboard (v0.6.1), shiny (v1.0.5)
Unattached non-base packages: Rcpp (v0.12.14), highr (v0.6), plyr (v1.8.4), compiler (v3.4.2), bindr (v0.1), bitops (v1.0-6), tools (v3.4.2), gtable (v0.2.0), jsonlite (v1.5), evaluate (v0.10.1), tibble (v1.3.4), pkgconfig (v2.0.1), rlang (v0.1.4), rstudioapi (v0.7), yaml (v2.1.14), parallel (v3.4.2), bindrcpp (v0.2), stringr (v1.2.0), xml2 (v1.1.1), revealjs (v0.9), grid (v3.4.2), rprojroot (v1.2), glue (v1.2.0), R6 (v2.2.2), rmarkdown (v1.8), bookdown (v0.5), RJSONIO (v1.3-0), magrittr (v1.5), scales (v0.5.0), backports (v1.1.1), htmltools (v0.3.6), rsconnect (v0.8.5), assertthat (v0.2.0), colorspace (v1.3-2), mime (v0.5), xtable (v1.8-2), httpuv (v1.3.5), RCurl (v1.95-4.8), lazyeval (v0.2.1), munsell (v0.4.3), slam (v0.1-40)
```




## References
